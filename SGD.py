# -*- coding: utf-8 -*-
"""data extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZS6r_517FywDVFTDU-m9RaWBSRUS2Qrq
"""

import numpy as np
import matplotlib.pyplot as plt

#Getting data from file
data = []
with open('boston.data') as f: #using the file from Sklearn's website
    raw_data = f.readlines()[22:]
for line in raw_data: #formaing the data into a 2d array
    trimmed_line =line[:len(line)-2].split()
    if raw_data.index(line)%2 !=0:
        data[-1].extend(trimmed_line)
    else:
        data.append(trimmed_line)
data = np.array(data, dtype='float32')
X = data[:,:13] # first 13 columns
Y = data[:,13] # only the 14th column


class NeuralNetwork():
  def __init__(self):
    #The architecture of the NN is changed here i.e adding/removing neurons or layers
    self.l1 = Layer(13,2)
    self.l2 = Layer(2,1)
    self.layers = [self.l1,self.l2]
    self.input = None

  def forward(self,x):
    self.input = x # input is needed for the backward pass of the final layer
    return ReLu(self.l2(ReLu(self.l1(x))))

  def backward(self,target,prediction):
    error = (prediction - target)
    delta = error * ReLu_derivative(self.layers[-1].output)

    for layer in reversed(list(self.layers)):
      if layer == self.layers[0]:
        layer.grad += delta * np.reshape(self.input,(1,len(self.input)))
        break
      prev_layer = self.layers[self.layers.index(layer)-1]
      layer.grad += delta * prev_layer.output
      delta = delta * prev_layer.weights

      if len(self.layers) > 2 and  self.layers.index(layer)-1!=0: # for shallower models adding the delta is not needed
        delta = np.reshape(np.sum(delta,axis=0),(1,len(prev_layer.bias))).T
        
     
  def  step(self,lr=0.001): 
    for layer in self.layers:
      layer.weights = layer.weights + (-lr*layer.grad)
      layer.grad = np.zeros_like(layer.weights) # gradiants are flushed after updating weights
      
      
      

class Layer():
  def __init__(self,in_shape,out_shape):
    #weights are decleared here from a normal distribution
    self.weights = np.random.normal(size=(in_shape,out_shape)).T
    self.bias = np.random.normal(size=(out_shape))
    self.net = None
    self.output = None
    self.grad = np.zeros_like(self.weights) # gradiants of the weights are stored here
  def __call__(self,x):
    self.net = np.dot(x,self.weights.T) + self.bias # this performs the forward pass
    self.output = np.copy(ReLu(self.net))
    return np.copy(self.net)


def ReLu(x):
  return np.maximum(0, x)

def ReLu_derivative(x):
  return np.greater(x, 0.).astype(np.float32)

def MSE_Error(pred,target):
  return ((target - pred)**2)/2

np.random.seed(2611) # seed for reproducability
model = NeuralNetwork()
x_train , y_train = X[:354], Y[:354]
x_test , y_test =  X[354:], Y[354:] #70% train 30% test

err = []
average_error = []
#hyperparameters
learning_rate = 0.00000001 #without extremely small learning rate the model does not converge
epochs = 800

#Training loop
for j in range(epochs):
  for epoch in range(len(x_train)):
    prediction = model.forward(x_train[epoch])
    model.backward(y_train[epoch],prediction)
    model.step(lr=learning_rate)
    Error = MSE_Error(prediction,y_train[epoch])
    err.append(Error)
  average_error.append(sum(err)/len(err))
  print('\n*********\n')
  print(f'Epoch: {j+1} / {epochs}\nAverage Loss: {average_error[j]}')
  print(f'Prediction: {prediction}\nActual: {y_train[353]}')
  print('\n*********\n')
  err = []
#plotting the error
plt.figure(figsize=(10,6))
plt.plot(average_error, c='red')
plt.title('Train Loss')
plt.xlabel("Epoch")
plt.ylabel("Error")
plt.show()

#Testing Loop
err = []
pred = []
for i in range(len(x_test)):
  prediction = model.forward(x_test[i])
  pred.append(prediction)
  Error = MSE_Error(prediction,y_test[i])
  err.append(Error)
#plotting the error
plt.figure(figsize=(10,6))
plt.plot(err)
plt.title('Test Loss')
plt.xlabel("Runs")
plt.ylabel("Error")
plt.legend(['Test Loss'])
plt.show()
print(f'Average Error = {sum(err)/len(err)}')
plt.plot(pred)
plt.plot(y_test)
plt.legend(['Predicted','Actual'])
plt.ylabel("Median House Price")
plt.show()